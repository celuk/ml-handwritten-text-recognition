{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYEfmwBRcNAo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'iam-handwriting-word-database:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1347338%2F2243895%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240325%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240325T022853Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0e3e466af5cae4af234422fd86d67d769e3baf2de996eb9f4cd13e81845cccfc96d8294030f721f9f588938974cb5797ec19b070f3fcdcbf92d6edf48f7acd1dc27bf592ecf61a0436ce7ea50402a471a053349a9598a987b15d805cde449c3cf05ade3761d753d7e78373b33b165509aeca56710f445bb1fa7b5b7fb25a836d26692ac53b629fdbd0d873b8d4c4a8d7e3cc3c1cd5d3420b808bff282b08be7b7b88057fa702de36aa885e2273090bb1edb23430bab85ecff810c0b8ceeefa4286bec7a20e6187223d4f9aee1f81e6c633d47f788d5987194b4bdcc3797716f5146da78d8dcb18ebfe91b044e3b03c198f82980912baa98f4b9f4d8e6d24f774'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "\n",
        "    # Check if the dataset already exists\n",
        "    if os.path.exists(destination_path):\n",
        "        print(f'Dataset already exists at {destination_path}, skipping download.')\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "metadata": {
        "id": "typoAtGXcOA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "RECORDS_COUNT = 30000"
      ],
      "metadata": {
        "id": "rGgQd2jCfWYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = []\n",
        "train_labels = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "train_original_text = []\n",
        "\n",
        "valid_images = []\n",
        "valid_labels = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_original_text = []\n",
        "\n",
        "inputs_length = []\n",
        "labels_length = []"
      ],
      "metadata": {
        "id": "P_zsYL_bfYsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/kaggle/input/iam-handwriting-word-database/iam_words/words.txt\") as f:\n",
        "    contents = f.readlines()[18:22539]\n",
        "\n",
        "lines = [line.strip() for line in contents]\n",
        "lines[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4I7JcUPbfaab",
        "outputId": "72ee7783-8f97-4ab9-d90a-e0143983142e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'b06-110-07-03 ok 173 987 2031 8 6 . .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(img):\n",
        "    \"\"\"\n",
        "    Converts image to shape (32, 128, 1) & normalize\n",
        "    \"\"\"\n",
        "    w, h = img.shape\n",
        "\n",
        "#     _, img = cv2.threshold(img,\n",
        "#                            128,\n",
        "#                            255,\n",
        "#                            cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "\n",
        "    # Aspect Ratio Calculation\n",
        "    new_w = 32\n",
        "    new_h = int(h * (new_w / w))\n",
        "    img = cv2.resize(img, (new_h, new_w))\n",
        "    w, h = img.shape\n",
        "\n",
        "    img = img.astype('float32')\n",
        "\n",
        "    # Converts each to (32, 128, 1)\n",
        "    if w < 32:\n",
        "        add_zeros = np.full((32-w, h), 255)\n",
        "        img = np.concatenate((img, add_zeros))\n",
        "        w, h = img.shape\n",
        "\n",
        "    if h < 128:\n",
        "        add_zeros = np.full((w, 128-h), 255)\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "        w, h = img.shape\n",
        "\n",
        "    if h > 128 or w > 32:\n",
        "        dim = (128,32)\n",
        "        img = cv2.resize(img, dim)\n",
        "\n",
        "    img = cv2.subtract(255, img)\n",
        "\n",
        "    img = np.expand_dims(img, axis=2)\n",
        "\n",
        "    # Normalize\n",
        "    img = img / 255\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "uvyt8PcLigya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_label_len = 0\n",
        "\n",
        "char_list = \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "# string.ascii_letters + string.digits (Chars & Digits)\n",
        "# or\n",
        "# \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "print(char_list, len(char_list))\n",
        "\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, chara in enumerate(txt):\n",
        "        dig_lst.append(char_list.index(chara))\n",
        "\n",
        "    return dig_lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f5kiaYGijBJ",
        "outputId": "bbc09ee3-77b6-4c29-d3ff-c6af7c877202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz 78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, line in enumerate(lines):\n",
        "    splits = line.split(' ')\n",
        "    status = splits[1]\n",
        "\n",
        "    if status == 'ok':\n",
        "        word_id = splits[0]\n",
        "        word = \"\".join(splits[8:])\n",
        "\n",
        "        splits_id = word_id.split('-')\n",
        "        filepath = 'iam-handwriting-word-database/iam_words/words/{}/{}-{}/{}.png'.format(splits_id[0],\n",
        "                                                  splits_id[0],\n",
        "                                                  splits_id[1],\n",
        "                                                  word_id)\n",
        "\n",
        "        # process image\n",
        "        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "        try:\n",
        "            img = process_image(img)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # process label\n",
        "        try:\n",
        "            label = encode_to_labels(word)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        if index % 10 == 0:\n",
        "            valid_images.append(img)\n",
        "            valid_labels.append(label)\n",
        "            valid_input_length.append(31)\n",
        "            valid_label_length.append(len(word))\n",
        "            valid_original_text.append(word)\n",
        "        else:\n",
        "            train_images.append(img)\n",
        "            train_labels.append(label)\n",
        "            train_input_length.append(31)\n",
        "            train_label_length.append(len(word))\n",
        "            train_original_text.append(word)\n",
        "\n",
        "        if len(word) > max_label_len:\n",
        "            max_label_len = len(word)\n",
        "\n",
        "    if index >= RECORDS_COUNT:\n",
        "        break"
      ],
      "metadata": {
        "id": "3jad9HqHimVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded_label = pad_sequences(train_labels,\n",
        "                             maxlen=max_label_len,\n",
        "                             padding='post',\n",
        "                             value=len(char_list))\n",
        "\n",
        "valid_padded_label = pad_sequences(valid_labels,\n",
        "                             maxlen=max_label_len,\n",
        "                             padding='post',\n",
        "                             value=len(char_list))"
      ],
      "metadata": {
        "id": "BicAzV6TipJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[3101]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "BpNUF-kNirvy",
        "outputId": "4d7153af-937f-424c-d68e-c27880ddc478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-5a5f947cd938>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3101\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded_label[3101]"
      ],
      "metadata": {
        "id": "IT81H5XuitoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded_label.shape, valid_padded_label.shape"
      ],
      "metadata": {
        "id": "VkhRdpRTitrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = np.asarray(train_images)\n",
        "train_input_length = np.asarray(train_input_length)\n",
        "train_label_length = np.asarray(train_label_length)\n",
        "\n",
        "valid_images = np.asarray(valid_images)\n",
        "valid_input_length = np.asarray(valid_input_length)\n",
        "valid_label_length = np.asarray(valid_label_length)"
      ],
      "metadata": {
        "id": "X4tNVbjLitt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import backend as tf_keras_backend\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "\n",
        "tf_keras_backend.set_image_data_format('channels_last')\n",
        "tf_keras_backend.image_data_format()"
      ],
      "metadata": {
        "id": "MN3DMlsoitwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Model1():\n",
        "    # input with shape of height=32 and width=128\n",
        "    inputs = Input(shape=(32,128,1))\n",
        "\n",
        "    # convolution layer with kernel size (3,3)\n",
        "    conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "    # poolig layer with kernel size (2,2)\n",
        "    pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        "\n",
        "    conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "    pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        "\n",
        "    conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        "\n",
        "    conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "    # poolig layer with kernel size (2,1)\n",
        "    pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        "\n",
        "    conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "    # Batch normalization layer\n",
        "    batch_norm_5 = BatchNormalization()(conv_5)\n",
        "\n",
        "    conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "    batch_norm_6 = BatchNormalization()(conv_6)\n",
        "    pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        "\n",
        "    conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        "\n",
        "    squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        "\n",
        "    # bidirectional LSTM layers with units=128\n",
        "    blstm_1 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "    blstm_2 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        "\n",
        "    outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "    # model to be used at test time\n",
        "    act_model = Model(inputs, outputs)\n",
        "\n",
        "    return act_model,outputs,inputs"
      ],
      "metadata": {
        "id": "QI2n-5Fii5CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act_model,outputs,inputs=Model1()"
      ],
      "metadata": {
        "id": "ld9ijM3pi6x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act_model.summary()"
      ],
      "metadata": {
        "id": "JzuF0rfEi7MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, the_labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, the_labels, input_length, label_length], outputs=loss_out)"
      ],
      "metadata": {
        "id": "Dq6db4sQi75H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5mi3jYvNi77d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJ49NoyLi790"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}