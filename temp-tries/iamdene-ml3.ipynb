{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cae4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 26813\n",
      "Total validation samples: 5746\n",
      "Total test samples: 5746\n",
      "Total: 38305\n",
      "Maximum length:  19\n",
      "Vocab size:  76\n",
      "['[UNK]', '!', '\"', '#', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "base_path = \"iam-handwriting-word-database\"\n",
    "words_list = []\n",
    "\n",
    "words = open(f\"{base_path}/words_new.txt\", \"r\").readlines()\n",
    "for line in words:\n",
    "    if line[0] == \"#\":\n",
    "        continue\n",
    "    if line.split(\" \")[1] != \"err\":  # We don't need to deal with errored entries.\n",
    "        words_list.append(line)\n",
    "\n",
    "len(words_list)\n",
    "\n",
    "np.random.shuffle(words_list)\n",
    "\n",
    "split_idx = int(0.7 * len(words_list))\n",
    "train_samples = words_list[:split_idx]\n",
    "test_samples = words_list[split_idx:]\n",
    "\n",
    "val_split_idx = int(0.5 * len(test_samples))\n",
    "validation_samples = test_samples[:val_split_idx]\n",
    "test_samples = test_samples[val_split_idx:]\n",
    "\n",
    "assert len(words_list) == len(train_samples) + len(validation_samples) + len(\n",
    "    test_samples\n",
    ")\n",
    "\n",
    "print(f\"Total training samples: {len(train_samples)}\")\n",
    "print(f\"Total validation samples: {len(validation_samples)}\")\n",
    "print(f\"Total test samples: {len(test_samples)}\")\n",
    "print(f\"Total: {len(train_samples) + len(validation_samples) + len(test_samples)}\")\n",
    "\n",
    "base_image_path = os.path.join(base_path, \"iam_words/words\")\n",
    "\n",
    "\n",
    "def get_image_paths_and_labels(samples):\n",
    "    paths = []\n",
    "    corrected_samples = []\n",
    "    for (i, file_line) in enumerate(samples):\n",
    "        line_split = file_line.strip()\n",
    "        line_split = line_split.split(\" \")\n",
    "\n",
    "        # Each line split will have this format for the corresponding image:\n",
    "        # part1/part1-part2/part1-part2-part3.png\n",
    "        image_name = line_split[0]\n",
    "        partI = image_name.split(\"-\")[0]\n",
    "        partII = image_name.split(\"-\")[1]\n",
    "        img_path = os.path.join(\n",
    "            base_image_path, partI, partI + \"-\" + partII, image_name + \".png\"\n",
    "        )\n",
    "        if os.path.getsize(img_path):\n",
    "            paths.append(img_path)\n",
    "            corrected_samples.append(file_line.split(\"\\n\")[0])\n",
    "\n",
    "    return paths, corrected_samples\n",
    "\n",
    "\n",
    "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
    "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
    "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)\n",
    "\n",
    "# Find maximum length and the size of the vocabulary in the training data.\n",
    "train_labels_cleaned = []\n",
    "characters = set()\n",
    "max_len = 0\n",
    "\n",
    "for label in train_labels:\n",
    "    label = label.split(\" \")[-1].strip()\n",
    "    for char in label:\n",
    "        characters.add(char)\n",
    "\n",
    "    max_len = max(max_len, len(label))\n",
    "    train_labels_cleaned.append(label)\n",
    "\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "print(\"Maximum length: \", max_len)\n",
    "print(\"Vocab size: \", len(characters))\n",
    "\n",
    "# Check some label samples.\n",
    "train_labels_cleaned[:10]\n",
    "\n",
    "def clean_labels(labels):\n",
    "    cleaned_labels = []\n",
    "    for label in labels:\n",
    "        label = label.split(\" \")[-1].strip()\n",
    "        cleaned_labels.append(label)\n",
    "    return cleaned_labels\n",
    "\n",
    "\n",
    "validation_labels_cleaned = clean_labels(validation_labels)\n",
    "test_labels_cleaned = clean_labels(test_labels)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Mapping characters to integers.\n",
    "char_to_num = StringLookup(vocabulary=list(characters), mask_token=None)\n",
    "\n",
    "# Mapping integers back to original characters.\n",
    "num_to_char = StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "print(char_to_num.get_vocabulary())\n",
    "\n",
    "def distortion_free_resize(image, img_size):\n",
    "    w, h = img_size\n",
    "    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n",
    "\n",
    "    # Check tha amount of padding needed to be done.\n",
    "    pad_height = h - tf.shape(image)[0]\n",
    "    pad_width = w - tf.shape(image)[1]\n",
    "\n",
    "    # Only necessary if you want to do same amount of padding on both sides.\n",
    "    if pad_height % 2 != 0:\n",
    "        height = pad_height // 2\n",
    "        pad_height_top = height + 1\n",
    "        pad_height_bottom = height\n",
    "    else:\n",
    "        pad_height_top = pad_height_bottom = pad_height // 2\n",
    "\n",
    "    if pad_width % 2 != 0:\n",
    "        width = pad_width // 2\n",
    "        pad_width_left = width + 1\n",
    "        pad_width_right = width\n",
    "    else:\n",
    "        pad_width_left = pad_width_right = pad_width // 2\n",
    "\n",
    "    image = tf.pad(\n",
    "        image,\n",
    "        paddings=[\n",
    "            [pad_height_top, pad_height_bottom],\n",
    "            [pad_width_left, pad_width_right],\n",
    "            [0, 0],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "batch_size = 64\n",
    "padding_token = 99\n",
    "image_width = 128\n",
    "image_height = 32\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, img_size=(image_width, image_height)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, 1)\n",
    "    image = distortion_free_resize(image, img_size)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "\"\"\"\n",
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.resize(image, [image_height, image_width])\n",
    "    image = tf.image.grayscale_to_rgb(image)  # Convert grayscale to RGB\n",
    "    image /= 255.0  # Normalize to [0,1]\n",
    "    return image\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def vectorize_label(label):\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    length = tf.shape(label)[0]\n",
    "    pad_amount = max_len - length\n",
    "    label = tf.pad(label, paddings=[[0, pad_amount]], constant_values=padding_token)\n",
    "    return label\n",
    "\n",
    "\n",
    "def process_images_labels(image_path, label):\n",
    "    image = preprocess_image(image_path)\n",
    "    label = vectorize_label(label)\n",
    "    return {\"image\": image, \"label\": label} #image, label #{\"image\": image, \"label\": label}\n",
    "\n",
    "\n",
    "def prepare_dataset(image_paths, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels)).map(\n",
    "        process_images_labels, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    return dataset.batch(batch_size).cache().prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = prepare_dataset(train_img_paths, train_labels_cleaned)\n",
    "validation_ds = prepare_dataset(validation_img_paths, validation_labels_cleaned)\n",
    "test_ds = prepare_dataset(test_img_paths, test_labels_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99037fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def segment_characters(image, num_boxes):\n",
    "    # Get the width and height of the image\n",
    "    total_width = image.shape[1]\n",
    "    box_height = image.shape[0]\n",
    "\n",
    "    # Calculate the width of each box\n",
    "    box_width = total_width // num_boxes\n",
    "\n",
    "    character_images = []\n",
    "    for i in range(num_boxes):\n",
    "        # Extract window from the image\n",
    "        start_x = i * box_width\n",
    "        end_x = (i + 1) * box_width\n",
    "        start_y = 0\n",
    "        end_y = box_height\n",
    "\n",
    "        char_image = image[start_y:end_y, start_x:end_x]\n",
    "\n",
    "        # Resize character image to a fixed size (e.g., 28x28)\n",
    "        char_image = cv2.resize(char_image, (28, 28))\n",
    "\n",
    "        character_images.append(char_image)\n",
    "\n",
    "    return character_images\n",
    "\n",
    "def save_characters_and_create_csv(images, labels, dataset_dir=\"character-dataset\"):\n",
    "    # Create dataset directory if it doesn't exist\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "\n",
    "    # Initialize a list to store filenames and labels\n",
    "    data = []\n",
    "\n",
    "    for image, label in zip(images, labels):\n",
    "        try:\n",
    "            # Create directory for this label if it doesn't exist\n",
    "            label_dir = os.path.join(dataset_dir, label)\n",
    "            if not os.path.exists(label_dir):\n",
    "                os.makedirs(label_dir)\n",
    "    \n",
    "            # Generate filename for this image\n",
    "            filenames = os.listdir(label_dir)\n",
    "            filename = f\"{len(filenames) + 1}-{label}.png\"\n",
    "            filepath = os.path.join(label_dir, filename)\n",
    "    \n",
    "            # Save image\n",
    "            im = Image.fromarray(image)\n",
    "            im.save(filepath)\n",
    "    \n",
    "            # Add filename and label to data\n",
    "            data.append({\"filename\": filepath, \"label\": label})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Create DataFrame and save as CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(os.path.join(dataset_dir, \"character-dataset.csv\"), index=False)\n",
    "\n",
    "# Segment characters and save them into corresponding directories\n",
    "for data in train_ds.take(1):\n",
    "    images, labels = data[\"image\"], data[\"label\"]\n",
    "    for image, label in zip(images, labels):\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        image = tf.transpose(image, perm=[1, 0, 2])\n",
    "        image = (image * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "        image = image[:, :, 0]\n",
    "        \n",
    "        indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))\n",
    "        # Convert to string.\n",
    "        label = tf.strings.reduce_join(num_to_char(indices))\n",
    "        label = label.numpy().decode(\"utf-8\")\n",
    "        \n",
    "        characters = segment_characters(image, len(label))\n",
    "        \n",
    "        save_characters_and_create_csv(characters, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af285f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae97e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f7f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11ff2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d2d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv('character-dataset/character-dataset.csv')\n",
    "\n",
    "# Load images and labels\n",
    "images = [np.array(Image.open(fname)) for fname in df['filename']]\n",
    "labels = df['label'].values\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Flatten images\n",
    "X_train = [x.flatten() for x in X_train]\n",
    "X_test = [x.flatten() for x in X_test]\n",
    "\n",
    "# Train a KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(f\"Model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836462b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbcd2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a96b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e5b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51917003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f711da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
